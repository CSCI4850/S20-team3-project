{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heat of the moment, when the enemy missiles are bearing down, a human being will utilize their learned abilities to react, and come out on top. Action games are a perfect environment for this learned ability to react to shine, and have been shown to improve players' perception, attention, and reaction time.[^1] We believe that by recreating the scenarios in which the human brain reacts, we can uncover how it learns to react. To do this, we will be using a Deep Q-Learning Network with Reinforcement Learning techniques to learn to play the 1981 Arcade Classic: Galaga.\n",
    "\n",
    "Utilizing the OpenAI Gym, and its Retro Gym environment additions, the network will learn to identify gameplay elements using convolutional layers, learn actions through reward, and establish a state-decision formula using Q-Learning. Our techniques may vary as development continues.\n",
    "\n",
    "Our network will be built using the tools provided by the Tensorflow and Tensorflow Keras python libraries. Our input data will be provided by the OpenAI Retro Gym, and includes full per-pixel data of the screen after an action is taken. Some of these observations are provided by the default Galaga environment, but we have the ability to add more observations as we find necessary. Our output data will consist of a vector mapping to one of five (5) actions in the Action Space, a one dimensional vector of Galaga's actions: Move Left, Move Right, Fire, and combinations of moving and firing. The OpenAI Retro Gym allows us a viewless, increased speed version of the game that can speed up training times.\n",
    "\n",
    "To support our DQN Agent, we will utilize a first-phase Deep Convolutional network to process the by-pixel observation and produce inputs for our second-phase DQN Agent that will determine which action to take.\n",
    "\n",
    "We intend to separate Galaga's 255 available levels into three interleaving sets for training, validation, and testing. Using a mix of difficult, simple, and special levels among the sets to ensure training, validation, and testing requirements are met, while also ensuring the Network has a proper variety of training data as to be able to meet any Galaga level.\n",
    "\n",
    "Galaga is a simple, straight-forward game. Its action space is small, and enemy actions are telegraphed. This makes Galaga simple to learn, but through its method of increasing enemy counts and firing rates, as well as introducing special enemies at points, Galaga is also a hard game to master.\n",
    "\n",
    "Our input data will consist of a by-pixel image of the screen a set number of frames, τ, after the last chosen action is executed. This image will be of height 224px, width 240 pixels, and grayscaled to exhibit only one value per pixel, resulting in an input size of 53,760. Research done in other DQN Agents shows that the use of RGB values over grayscale values does not produce greater network performance, and is longer to train due to the 3x larger input size[^2].\n",
    "\n",
    "Further required input data will be determined as development continues.\n",
    "\n",
    "Each fit of the model would cover one playthrough of the game, beginning at the first level of the training set and continuing until the unit is out of lives or has completed the final level of the training set.\n",
    "\n",
    "Evaluation of the model would cover one playthrough of the game, beginning at the first level of the testing set and continuing until the unit is out of lives or has completed the final level of the testing set.\n",
    "\n",
    "Further training possibilities would include various \"scenarios\" in which the agent is trained with an extra restriction, such as time-to-kill enemies, or accuracy.\n",
    "\n",
    "Comparative evaluation assessment would be done utilizing:\n",
    "- Past high scores\n",
    "- Global high score list (human players)\n",
    "- Global high score list (other neural networks) !(In whatever quantity these are found available\n",
    "- Etcetera Galaga stats about performance\n",
    "    \n",
    "References:\n",
    "1. Nauert, R. (2018, August 8). Action Video Games May Improve Cognitive Skills. Retrieved February 13, 2020, from https://psychcentral.com/news/2017/12/13/action-video-games-may-improve-cognitive-skills/129902.html\n",
    "2. Dhoot, Tushar, Daniyal Kahn, and Ben Konyi. “Playing DOOM Using Deep Reinforcement Learning.” Accessed February 26, 2020. http://cs229.stanford.edu/proj2017/final-reports/5238810.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
