{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heat of the moment, when the enemy missiles are bearing down, a human being will utilize their learned abilities to react, and come out on top. Action games are a perfect environment for this learned ability to react to shine, and have been shown to improve players' perception, attention, and reaction time.[^1] We believe that by recreating the scenarios in which the human brain reacts, we can uncover how it learns to react. To do this, we will be using a Deep Q-Learning Network with Reinforcement Learning techniques to learn to play the 1981 Arcade Classic: Galaga.\n",
    "\n",
    "We will be utilizing the Deep Q-Learning architecture (Minh et al., 2015), an extension of the classic Q-Learning mechanism (Watkins, 1989), which utilized a policy-iteration technique around potential rewards to develop the correct action response for a scenario. Further, we will introduced two enhancements to this architecture: Prioritized Memory Replay (Schaul et al., 2015) and Double Q-Learning (van Hasselt et al., 2016). These enhancements result in a five-standard comparison, that of: Random, DQN with Uniform Memory Replay, DQN with Prioritized Memory Replay, Double DQN with Uniform Memory Replay, and Double DQN with Prioritized memory replay.\n",
    "\n",
    "Utilizing the OpenAI Gym, and its Retro Gym environment additions, the network will learn to identify gameplay elements using convolutional layers, learn actions through reward, and establish a state-decision formula using Q-Learning. Our techniques may vary as development continues.\n",
    "\n",
    "Our network will be built using the tools provided by the Tensorflow and Tensorflow Keras python libraries. Our input data will be provided by the OpenAI Retro Gym, and includes full per-pixel data of the screen after an action is taken. Some of these observations are provided by the default Galaga environment, but we have the ability to add more observations as we find necessary. Our output data will consist of a vector mapping to one of five (5) actions in the Action Space, a one dimensional vector of Galaga's actions: Move Left, Move Right, Fire, and combinations of moving and firing. The OpenAI Retro Gym allows us a viewless, increased speed version of the game that can speed up training times.\n",
    "\n",
    "To support our DQN Agent, we will utilize a first-phase Deep Convolutional network to process the by-pixel observation and produce inputs for our second-phase DQN Agent that will determine which action to take.\n",
    "\n",
    "Galaga is a simple, straight-forward game. Its action space is small, and enemy actions are telegraphed. This makes Galaga simple to learn, but through its method of increasing enemy counts and firing rates, as well as introducing special enemies at points, Galaga is also a hard game to master.\n",
    "\n",
    "Our input data will consist of a by-pixel image of the screen a set number of frames, τ, after the last chosen action is executed. This image will be of height 100px, width 100px, and grayscaled to exhibit only one value per pixel, resulting in an input size of 10,000. Research done in other DQN Agents shows that the use of RGB values over grayscale values does not produce greater network performance, and is longer to train due to the 3x larger input size[^2].\n",
    "\n",
    "Each fit of the model would cover one playthrough of the game, beginning at the first level and continuing until the unit is out of lives or has completed the final level, but also may be restricted to a set number of frames to reduce training time.\n",
    "\n",
    "Evaluation of the model would cover 100 playthroughs with a frame limit, and the average score across these would be its comparative value.\n",
    "\n",
    "Each model will be trained and evaluated under the same hyperparameters and limits, as to allow for a direct comparison of all models.\n",
    "\n",
    "Further comparative evaluation assessment could also be done utilizing:\n",
    "- Global high score list (human players)\n",
    "- Global high score list (other neural networks) !(In whatever quantity these are found available)\n",
    "- Etcetera Galaga stats about performance\n",
    "    \n",
    "References:\n",
    "1. Nauert, R. (2018, August 8). Action Video Games May Improve Cognitive Skills. Retrieved February 13, 2020, from https://psychcentral.com/news/2017/12/13/action-video-games-may-improve-cognitive-skills/129902.html\n",
    "2. Dhoot, Tushar, Daniyal Kahn, and Ben Konyi. “Playing DOOM Using Deep Reinforcement Learning.” Accessed February 26, 2020. http://cs229.stanford.edu/proj2017/final-reports/5238810.pdf.\n",
    "\n",
    "H. van Hasselt, A. Guez, D. Silver. Deep Reinforcement Learning with Double Q-Learning, 2016.\n",
    "\n",
    "V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-vski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis.   Human-level control through deep reinforcement learning.Nature, 518(7540):529–533, 2015.\n",
    "\n",
    "T. Schaul, J. Quan, I. Antonoglou, D. Silver. Prioritized Experience Replay, 2016.\n",
    "\n",
    "C. J. C. H. Watkins.Learning from delayed rewards.  PhD thesis,University of Cambridge England, 1989."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
